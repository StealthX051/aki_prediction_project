{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AKI Prediction Project - Results Generation\n",
        "\n",
        "This notebook generates the final results tables and plots for the AKI Prediction Project.\n",
        "It uses the `results_analysis` module to process the `predictions.csv` files generated by `step_08_generate_predictions.py`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Add project root to path\n",
        "PROJECT_ROOT = Path('.').resolve().parent\n",
        "sys.path.append(str(PROJECT_ROOT))\n",
        "\n",
        "from results_recreation import results_analysis\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load and Calibrate Predictions\n",
        "We load all `predictions.csv` files found in the `results/models` directory.\n",
        "Then, we apply **Logistic Regression Calibration** (Platt Scaling) using 5-fold CV on the test set predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = results_analysis.load_all_predictions()\n",
        "print(f\"Loaded {len(df)} predictions.\")\n",
        "\n",
        "if not df.empty:\n",
        "    df_calib = results_analysis.calibrate_predictions(df)\n",
        "    print(\"Calibration complete.\")\n",
        "else:\n",
        "    print(\"No data found. Please run step_08_generate_predictions.py first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Calculate Metrics\n",
        "We calculate standard performance metrics (AUROC, AUPRC, Brier, F1, etc.) for each model configuration.\n",
        "We use **F2-score maximization** to find the optimal decision threshold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not df.empty:\n",
        "    metrics_df = results_analysis.calculate_metrics(df_calib)\n",
        "    display(metrics_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Generate HTML Tables\n",
        "We generate styled HTML tables for each outcome and branch, highlighting the best performing models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not df.empty:\n",
        "    results_analysis.generate_html_tables(metrics_df)\n",
        "    print(\"HTML tables saved to results/tables/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Plot Curves\n",
        "We generate ROC, Precision-Recall, and Calibration curves for each outcome/branch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not df.empty:\n",
        "    results_analysis.plot_curves(df_calib)\n",
        "    print(\"Plots saved to results/figures/\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
