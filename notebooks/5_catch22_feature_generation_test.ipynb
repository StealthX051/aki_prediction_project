{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192021be",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m logging\u001b[38;5;241m.\u001b[39mbasicConfig(level\u001b[38;5;241m=\u001b[39mlogging\u001b[38;5;241m.\u001b[39mINFO, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%(asctime)s\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m%(levelname)s\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m%(message)s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# --- Paths ---\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m PROJECT_ROOT \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(\u001b[38;5;18;43m__file__\u001b[39;49m)))\n\u001b[0;32m     35\u001b[0m PROCESSED_DATA_DIR \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(PROJECT_ROOT, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocessed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     36\u001b[0m RAW_DATA_DIR \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(PROJECT_ROOT, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 7_generate_catch22_features_sliding_window.py\n",
    "#\n",
    "# Description:\n",
    "# This script implements the \"Sliding Window Statistics\" approach to address\n",
    "# the weak supervision problem. Instead of one feature set per surgery, it\n",
    "# analyzes the waveform in windows (e.g., 10-minute windows, sliding by 5 mins).\n",
    "# It calculates catch22 features for each window, then computes summary\n",
    "# statistics (mean, std, min, max) across all windows for each feature.\n",
    "# The result is a richer feature set per patient that captures the dynamics\n",
    "# and volatility of the intraoperative period.\n",
    "#\n",
    "# =============================================================================\n",
    "\n",
    "# --- Standard Library Imports ---\n",
    "import os\n",
    "import logging\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import partial\n",
    "\n",
    "# --- Third-Party Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import vitaldb\n",
    "import pycatch22\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =============================================================================\n",
    "# 1. CONFIGURATION\n",
    "# =============================================================================\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "# --- Paths ---\n",
    "# ** FIXED: Robust pathing for both script and notebook execution **\n",
    "try:\n",
    "    # This works when running as a .py file\n",
    "    PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
    "except NameError:\n",
    "    # This works when running in a Jupyter Notebook or other interactive environment\n",
    "    PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "\n",
    "PROCESSED_DATA_DIR = os.path.join(PROJECT_ROOT, \"data\", \"processed\")\n",
    "RAW_DATA_DIR = os.path.join(PROJECT_ROOT, \"data\", \"raw\")\n",
    "\n",
    "# --- File Names ---\n",
    "COHORT_FILE = \"final_cohort_with_death_label.csv\"\n",
    "OUTPUT_FILE = \"waveform_catch22_features_sliding_window.csv\"\n",
    "\n",
    "# --- Parameters ---\n",
    "WAVEFORM_NAME = 'SNUADC/PLETH'\n",
    "# ** FIXED: Using the correct, documented original sampling rate **\n",
    "ORIGINAL_SR = 500  # SNUADC/PLETH is documented as 500Hz\n",
    "DOWNSAMPLED_SR = 10  # Target sample rate of 10Hz\n",
    "\n",
    "# Sliding window parameters, chosen to balance detail and computation\n",
    "WINDOW_SIZE_SECONDS = 10 * 60  # 10 minutes\n",
    "SLIDE_SECONDS = 5 * 60         # 5 minutes\n",
    "WINDOW_SIZE_SAMPLES = WINDOW_SIZE_SECONDS * DOWNSAMPLED_SR\n",
    "SLIDE_SAMPLES = SLIDE_SECONDS * DOWNSAMPLED_SR\n",
    "\n",
    "# =============================================================================\n",
    "# 2. WORKER FUNCTION FOR PARALLEL PROCESSING\n",
    "# =============================================================================\n",
    "def process_case(case_info):\n",
    "    \"\"\"\n",
    "    Worker function to process a single case. Loads waveform, slices into\n",
    "    windows, extracts catch22 features for each window, and returns\n",
    "    summary statistics across all windows.\n",
    "    \"\"\"\n",
    "    caseid, opstart, opend = case_info\n",
    "    try:\n",
    "        # ** CRITICAL FIX: Use vitaldb.load_case(), the correct high-level function. **\n",
    "        # This function handles finding, downloading, and caching the data.\n",
    "        # We request the data at its original 500Hz resolution.\n",
    "        wave = vitaldb.load_case(caseid, WAVEFORM_NAME, 1 / ORIGINAL_SR)\n",
    "\n",
    "        if wave is None or len(wave) == 0:\n",
    "            return {'caseid': caseid, 'error': 'Waveform is empty or None'}\n",
    "\n",
    "        # Dynamic calculation of downsample factor\n",
    "        DOWNSAMPLE_FACTOR = ORIGINAL_SR // DOWNSAMPLED_SR\n",
    "        \n",
    "        # Convert opstart/opend times (in seconds) to sample indices\n",
    "        start_index = int(opstart * ORIGINAL_SR)\n",
    "        end_index = int(opend * ORIGINAL_SR)\n",
    "        intraop_wave = wave[start_index:end_index]\n",
    "        \n",
    "        # Validate that the waveform is long enough for at least one window\n",
    "        if len(intraop_wave) < (WINDOW_SIZE_SECONDS * ORIGINAL_SR):\n",
    "            return {'caseid': caseid, 'error': 'Waveform shorter than one window'}\n",
    "            \n",
    "        downsampled_wave = intraop_wave[::DOWNSAMPLE_FACTOR]\n",
    "\n",
    "        window_features_list = []\n",
    "        # --- Sliding Window Loop ---\n",
    "        for i in range(0, len(downsampled_wave) - WINDOW_SIZE_SAMPLES + 1, SLIDE_SAMPLES):\n",
    "            window = downsampled_wave[i : i + WINDOW_SIZE_SAMPLES]\n",
    "            \n",
    "            # Skip invalid windows (e.g., flatline or all NaNs)\n",
    "            if np.nanstd(window) < 1e-6 or np.all(np.isnan(window)):\n",
    "                continue \n",
    "                \n",
    "            features = pycatch22.catch22_all(window, catch24=False)\n",
    "            window_features_list.append(features)\n",
    "        \n",
    "        if not window_features_list:\n",
    "            return {'caseid': caseid, 'error': 'No valid windows found'}\n",
    "\n",
    "        # --- Aggregate Features ---\n",
    "        windows_df = pd.DataFrame(window_features_list)\n",
    "        \n",
    "        stats_mean = windows_df.mean().add_suffix('_mean')\n",
    "        stats_std = windows_df.std().add_suffix('_std')\n",
    "        stats_min = windows_df.min().add_suffix('_min')\n",
    "        stats_max = windows_df.max().add_suffix('_max')\n",
    "        \n",
    "        final_features = pd.concat([stats_mean, stats_std, stats_min, stats_max])\n",
    "        \n",
    "        result = final_features.to_dict()\n",
    "        result['caseid'] = caseid\n",
    "        return result\n",
    "\n",
    "    except Exception as e:\n",
    "        return {'caseid': caseid, 'error': str(e)}\n",
    "\n",
    "# =============================================================================\n",
    "# 3. MAIN EXECUTION\n",
    "# =============================================================================\n",
    "def main():\n",
    "    \"\"\"Main function to orchestrate the feature extraction.\"\"\"\n",
    "    logging.info(\"Starting sliding window catch22 feature extraction...\")\n",
    "\n",
    "    try:\n",
    "        cohort_df = pd.read_csv(os.path.join(PROCESSED_DATA_DIR, COHORT_FILE))\n",
    "        logging.info(f\"Loaded {len(cohort_df)} cases from {COHORT_FILE}\")\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"FATAL: Cohort file not found at {os.path.join(PROCESSED_DATA_DIR, COHORT_FILE)}\")\n",
    "        return\n",
    "    \n",
    "    cases_to_process = list(zip(cohort_df['caseid'], cohort_df['opstart'], cohort_df['opend']))\n",
    "\n",
    "    num_processes = max(1, cpu_count() - 1)\n",
    "    logging.info(f\"Using {num_processes} processes for extraction.\")\n",
    "    \n",
    "    all_features = []\n",
    "    with Pool(processes=num_processes) as pool:\n",
    "        with tqdm(total=len(cases_to_process), desc=\"Extracting Sliding Window Features\") as pbar:\n",
    "            for result in pool.imap_unordered(process_case, cases_to_process):\n",
    "                all_features.append(result)\n",
    "                pbar.update()\n",
    "\n",
    "    features_df = pd.DataFrame(all_features)\n",
    "\n",
    "    error_df = features_df[features_df['error'].notna()]\n",
    "    success_df = features_df[features_df['error'].isna()].drop(columns=['error'])\n",
    "    \n",
    "    if not error_df.empty:\n",
    "        logging.warning(f\"Encountered {len(error_df)} errors during processing.\")\n",
    "        error_log_path = os.path.join(PROCESSED_DATA_DIR, \"catch22_sliding_window_errors.csv\")\n",
    "        error_df.to_csv(error_log_path, index=False)\n",
    "        logging.warning(f\"Error log saved to {error_log_path}\")\n",
    "\n",
    "    if not success_df.empty:\n",
    "        # Fill potential NaN values from std dev calculation with 0\n",
    "        success_df.fillna(0, inplace=True)\n",
    "\n",
    "        output_path = os.path.join(PROCESSED_DATA_DIR, OUTPUT_FILE)\n",
    "        # Reorder columns to have caseid first for readability\n",
    "        cols = ['caseid'] + [col for col in success_df.columns if col != 'caseid']\n",
    "        success_df = success_df[cols]\n",
    "        success_df.to_csv(output_path, index=False)\n",
    "        \n",
    "        logging.info(f\"Successfully extracted features for {len(success_df)} cases.\")\n",
    "        logging.info(f\"Features saved to {output_path}\")\n",
    "    else:\n",
    "        logging.error(\"No features were successfully extracted. Check error log.\")\n",
    "        \n",
    "    logging.info(\"Feature extraction complete.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aki_prediction_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
