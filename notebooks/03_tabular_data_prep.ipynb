{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be05f136",
   "metadata": {},
   "source": [
    "# Data Wrangler Script\n",
    "\n",
    "Converts long feature file into wide feature file so we can train on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6de2e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Part 1: Comprehensive Feature Wrangler ---\n",
      "This script will clean preoperative data and merge it with waveform features.\n",
      "Project Root:        d:\\Projects\\aki_prediction_project\n",
      "Cohort (Preop) file: d:\\Projects\\aki_prediction_project\\data\\processed\\aki_pleth_ecg_co2_awp.csv\n",
      "Waveform (long) file:d:\\Projects\\aki_prediction_project\\data\\processed\\aki_pleth_ecg_co2_awp_inf.csv\n",
      "Output (wide) file:  d:\\Projects\\aki_prediction_project\\data\\processed\\aki_features_master_wide.csv\n",
      "Defined 25 preop, 15 continuous, 8 categorical features.\n",
      "Outlier handling function defined.\n",
      "\n",
      "--- Processing Preoperative Data ---\n",
      "Loading cohort file from d:\\Projects\\aki_prediction_project\\data\\processed\\aki_pleth_ecg_co2_awp.csv...\n",
      "Successfully loaded. Shape: (3462, 75)\n",
      "Selected 25 columns. Shape: (3462, 25)\n",
      "Splitting preoperative data into train/test sets for safe processing...\n",
      "Train set shape: (2769, 23), Test set shape: (693, 23)\n",
      "Processing categorical variables...\n",
      "Applying one-hot encoding...\n",
      "Created 34 one-hot encoded features.\n",
      "Handling outliers on continuous variables...\n",
      "Imputing all remaining missing values with -99...\n",
      "Missing values in train after imputation: 0\n",
      "Missing values in test after imputation: 0\n",
      "Recombining processed preoperative data...\n",
      "Final processed preoperative data shape: (3462, 51)\n",
      "\n",
      "--- Processing Waveform Data ---\n",
      "Loading long-format data from d:\\Projects\\aki_prediction_project\\data\\processed\\aki_pleth_ecg_co2_awp_inf.csv...\n",
      "Successfully loaded. Shape: (13661, 27)\n",
      "Pivoting waveform data from long to wide format...\n",
      "Pivot complete. Shape before flattening: (3447, 96)\n",
      "Flattening and cleaning column names...\n",
      "Column names flattened. Shape after reset_index: (3447, 98)\n",
      "Imputing missing waveform values (from missing windows) with 0...\n",
      "Total NaN values filled: 3048\n",
      "\n",
      "--- Merging Processed Data ---\n",
      "Merging waveform data (shape: (3447, 98)) with preop data (shape: (3462, 51))...\n",
      "Final master DataFrame shape: (3447, 147)\n",
      "\n",
      "Saving new master wide-format file to d:\\Projects\\aki_prediction_project\\data\\processed\\aki_features_master_wide.csv...\n",
      "---\n",
      "✅ Comprehensive data wrangler script complete.\n",
      "\n",
      "--- Final Output Sample (first 7 columns) ---\n",
      "   caseid  aki_label  Primus_AWP_CO_Embed2_Dist_tau_d_expfit_meandiff  \\\n",
      "0       1     0.0000                                           0.3683   \n",
      "1       2     0.0000                                           0.2915   \n",
      "2       4     0.0000                                           0.3399   \n",
      "3      10     0.0000                                           0.4937   \n",
      "4      13     0.0000                                           0.4287   \n",
      "\n",
      "   Primus_CO2_CO_Embed2_Dist_tau_d_expfit_meandiff  \\\n",
      "0                                           0.9568   \n",
      "1                                           0.8152   \n",
      "2                                           0.7843   \n",
      "3                                           0.4651   \n",
      "4                                           0.8350   \n",
      "\n",
      "   SNUADC_ECG_II_CO_Embed2_Dist_tau_d_expfit_meandiff  \\\n",
      "0                                             0.0544    \n",
      "1                                             0.0575    \n",
      "2                                             0.0387    \n",
      "3                                             0.0157    \n",
      "4                                             0.0211    \n",
      "\n",
      "   SNUADC_PLETH_CO_Embed2_Dist_tau_d_expfit_meandiff  \\\n",
      "0                                             0.1862   \n",
      "1                                             0.1731   \n",
      "2                                             0.1131   \n",
      "3                                             0.1773   \n",
      "4                                             0.0283   \n",
      "\n",
      "   Primus_AWP_CO_FirstMin_ac  \n",
      "0                    25.0000  \n",
      "1                    20.0000  \n",
      "2                    21.0000  \n",
      "3                    20.0000  \n",
      "4                    23.0000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"--- Part 1: Comprehensive Feature Wrangler ---\")\n",
    "print(\"This script will clean preoperative data and merge it with waveform features.\")\n",
    "\n",
    "# --- 1. Configuration and Paths ---\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# --- Path Configuration ---\n",
    "try:\n",
    "    PROJECT_ROOT = Path.cwd().parent\n",
    "    PROCESSED_DIR = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "    PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "except Exception as e:\n",
    "    print(f\"Error setting up paths: {e}\")\n",
    "    print(\"Please ensure you are running this from a subdirectory of your project root.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Input Files\n",
    "COHORT_FILE = PROCESSED_DIR / 'aki_pleth_ecg_co2_awp.csv' # From cohort generation\n",
    "LONG_WAVEFORM_FILE = PROCESSED_DIR / 'aki_pleth_ecg_co2_awp_inf.csv' # From catch22 prep\n",
    "\n",
    "# Output File\n",
    "WIDE_FEATURES_FILE = PROCESSED_DIR / 'aki_features_master_wide.csv'\n",
    "\n",
    "# Target Column\n",
    "TARGET_COLUMN = 'aki_label'\n",
    "\n",
    "print(f\"Project Root:        {PROJECT_ROOT}\")\n",
    "print(f\"Cohort (Preop) file: {COHORT_FILE}\")\n",
    "print(f\"Waveform (long) file:{LONG_WAVEFORM_FILE}\")\n",
    "print(f\"Output (wide) file:  {WIDE_FEATURES_FILE}\")\n",
    "\n",
    "# --- 2. Feature Definitions ---\n",
    "\n",
    "# All preoperative features to be selected from the cohort file\n",
    "PREOP_FEATURES_TO_SELECT = [\n",
    "    'caseid', TARGET_COLUMN, 'age', 'sex', 'emop', 'department', 'bmi', 'approach',\n",
    "    'preop_htn', 'preop_dm', 'preop_ecg', 'preop_pft', 'preop_hb', 'preop_plt',\n",
    "    'preop_pt', 'preop_aptt', 'preop_na', 'preop_k', 'preop_gluc', 'preop_alb',\n",
    "    'preop_ast', 'preop_alt', 'preop_bun', 'preop_cr', 'preop_hco3'\n",
    "]\n",
    "\n",
    "# Continuous features for outlier handling\n",
    "CONTINUOUS_COLS = [\n",
    "    'age', 'bmi', 'preop_hb', 'preop_plt', 'preop_pt', 'preop_aptt', 'preop_na',\n",
    "    'preop_k', 'preop_gluc', 'preop_alb', 'preop_ast', 'preop_alt',\n",
    "    'preop_bun', 'preop_cr', 'preop_hco3'\n",
    "    # Note: 'height' and 'weight' are not in the select list, so they are not here\n",
    "]\n",
    "\n",
    "# Categorical features for merging and one-hot encoding\n",
    "CATEGORICAL_COLS = [\n",
    "    'sex', 'emop', 'department', 'approach', 'preop_htn', 'preop_dm',\n",
    "    'preop_ecg', 'preop_pft'\n",
    "]\n",
    "\n",
    "# Waveform prefixes for later logic (used to separate preop vs waveform features)\n",
    "WAVEFORM_PREFIXES = ['SNUADC_PLETH', 'SNUADC_ECG_II', 'Primus_CO2', 'Primus_AWP']\n",
    "\n",
    "print(f\"Defined {len(PREOP_FEATURES_TO_SELECT)} preop, {len(CONTINUOUS_COLS)} continuous, {len(CATEGORICAL_COLS)} categorical features.\")\n",
    "\n",
    "# --- 3. Outlier Handling Function ---\n",
    "# This is the exact function from your plan\n",
    "def handle_outliers(df, train_df, continuous_cols):\n",
    "    \"\"\"\n",
    "    Handles outliers based on training set percentiles.\n",
    "    Replaces outliers with random values from a plausible range.\n",
    "    \"\"\"\n",
    "    df_processed = df.copy()\n",
    "    for col in continuous_cols:\n",
    "        if col in df_processed.columns:\n",
    "            \n",
    "            # 1. Create a clean, numeric version of the training data column\n",
    "            train_col_numeric = pd.to_numeric(train_df[col], errors='coerce')\n",
    "            \n",
    "            # 2. Force the column in the dataframe being processed to also be numeric\n",
    "            df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce')\n",
    "\n",
    "            # Calculate percentile thresholds ONLY from the numeric training column\n",
    "            # We must fill any new NaNs created by 'coerce' before calculating percentiles\n",
    "            train_col_numeric.dropna(inplace=True)\n",
    "            if train_col_numeric.empty:\n",
    "                print(f\"Warning: No valid data for '{col}' in training set. Skipping outlier handling for this column.\")\n",
    "                continue\n",
    "\n",
    "            low_p_0_5, low_p_1, low_p_5 = np.percentile(train_col_numeric, [0.5, 1, 5])\n",
    "            high_p_95, high_p_99_5 = np.percentile(train_col_numeric, [95, 99.5])\n",
    "            \n",
    "            # Identify outlier indices in the processed dataframe\n",
    "            low_outlier_indices = df_processed[df_processed[col] < low_p_1].index\n",
    "            high_outlier_indices = df_processed[df_processed[col] > high_p_99_5].index\n",
    "            \n",
    "            # Replace with random values from the specified plausible range\n",
    "            low_replacements = np.random.uniform(low_p_0_5, low_p_5, size=len(low_outlier_indices))\n",
    "            high_replacements = np.random.uniform(high_p_95, high_p_99_5, size=len(high_outlier_indices))\n",
    "            \n",
    "            df_processed.loc[low_outlier_indices, col] = low_replacements\n",
    "            df_processed.loc[high_outlier_indices, col] = high_replacements\n",
    "    return df_processed\n",
    "\n",
    "print(\"Outlier handling function defined.\")\n",
    "\n",
    "# --- 4. Process Preoperative Data ---\n",
    "print(\"\\n--- Processing Preoperative Data ---\")\n",
    "\n",
    "# 4.1 Load and Select Preop Data\n",
    "print(f\"Loading cohort file from {COHORT_FILE}...\")\n",
    "try:\n",
    "    cohort_df = pd.read_csv(COHORT_FILE)\n",
    "    print(f\"Successfully loaded. Shape: {cohort_df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Cohort file not found at {COHORT_FILE}\")\n",
    "    sys.exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Select only the columns we need\n",
    "try:\n",
    "    preop_df = cohort_df[PREOP_FEATURES_TO_SELECT].copy()\n",
    "    print(f\"Selected {len(PREOP_FEATURES_TO_SELECT)} columns. Shape: {preop_df.shape}\")\n",
    "except KeyError as e:\n",
    "    print(f\"ERROR: A column is missing from {COHORT_FILE}. Missing key: {e}\")\n",
    "    print(\"Please check PREOP_FEATURES_TO_SELECT against the cohort file columns.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# 4.2 Split Data (to prevent leakage)\n",
    "print(\"Splitting preoperative data into train/test sets for safe processing...\")\n",
    "X_preop = preop_df.drop(columns=[TARGET_COLUMN, 'caseid'])\n",
    "y_preop = preop_df[TARGET_COLUMN]\n",
    "caseid_series = preop_df['caseid']\n",
    "\n",
    "X_train, X_test, y_train, y_test, caseid_train, caseid_test = train_test_split(\n",
    "    X_preop, y_preop, caseid_series,\n",
    "    test_size=0.2, # Same split as the trainer script\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y_preop\n",
    ")\n",
    "print(f\"Train set shape: {X_train.shape}, Test set shape: {X_test.shape}\")\n",
    "\n",
    "# 4.3 Handle Categorical Data\n",
    "print(\"Processing categorical variables...\")\n",
    "# Merge 'department' counts < 30\n",
    "dept_counts = X_train['department'].value_counts()\n",
    "depts_to_merge = dept_counts[dept_counts < 30].index.tolist()\n",
    "\n",
    "if depts_to_merge:\n",
    "    print(f\"Merging {len(depts_to_merge)} departments into 'other': {depts_to_merge}\")\n",
    "    X_train['department'] = X_train['department'].replace(depts_to_merge, 'other')\n",
    "    X_test['department'] = X_test['department'].replace(depts_to_merge, 'other')\n",
    "\n",
    "# One-Hot Encoding\n",
    "print(\"Applying one-hot encoding...\")\n",
    "X_train_dummies = pd.get_dummies(X_train[CATEGORICAL_COLS], drop_first=True, dtype=int)\n",
    "X_test_dummies = pd.get_dummies(X_test[CATEGORICAL_COLS], drop_first=True, dtype=int)\n",
    "\n",
    "# Align columns - this is critical\n",
    "X_train_aligned, X_test_aligned = X_train_dummies.align(\n",
    "    X_test_dummies, join='left', axis=1, fill_value=0\n",
    ")\n",
    "print(f\"Created {X_train_aligned.shape[1]} one-hot encoded features.\")\n",
    "\n",
    "# Drop original categorical columns and add encoded ones\n",
    "X_train = X_train.drop(columns=CATEGORICAL_COLS)\n",
    "X_test = X_test.drop(columns=CATEGORICAL_COLS)\n",
    "\n",
    "X_train = pd.concat([X_train, X_train_aligned], axis=1)\n",
    "X_test = pd.concat([X_test, X_test_aligned], axis=1)\n",
    "\n",
    "# 4.4 Handle Outliers\n",
    "print(\"Handling outliers on continuous variables...\")\n",
    "X_train_cleaned = handle_outliers(X_train, X_train, CONTINUOUS_COLS)\n",
    "X_test_cleaned = handle_outliers(X_test, X_train, CONTINUOUS_COLS) # Use X_train stats\n",
    "\n",
    "# 4.5 Handle Missing Data (Imputation)\n",
    "print(\"Imputing all remaining missing values with -99...\")\n",
    "X_train_imputed = X_train_cleaned.fillna(-99)\n",
    "X_test_imputed = X_test_cleaned.fillna(-99)\n",
    "\n",
    "print(f\"Missing values in train after imputation: {X_train_imputed.isnull().sum().sum()}\")\n",
    "print(f\"Missing values in test after imputation: {X_test_imputed.isnull().sum().sum()}\")\n",
    "\n",
    "# 4.6 Recombine Processed Preop Data\n",
    "print(\"Recombining processed preoperative data...\")\n",
    "X_preop_processed = pd.concat([X_train_imputed, X_test_imputed])\n",
    "\n",
    "# Re-attach IDs and labels\n",
    "preop_final_df = pd.DataFrame({\n",
    "    'caseid': pd.concat([caseid_train, caseid_test]),\n",
    "    TARGET_COLUMN: pd.concat([y_train, y_test])\n",
    "}).reset_index(drop=True)\n",
    "\n",
    "# Align indexes for concat\n",
    "X_preop_processed = X_preop_processed.reset_index(drop=True)\n",
    "preop_final_df = pd.concat([preop_final_df, X_preop_processed], axis=1)\n",
    "\n",
    "print(f\"Final processed preoperative data shape: {preop_final_df.shape}\")\n",
    "\n",
    "# --- 5. Process Waveform Data ---\n",
    "print(\"\\n--- Processing Waveform Data ---\")\n",
    "\n",
    "# 5.1 Load Long-Format Waveform Data\n",
    "print(f\"Loading long-format data from {LONG_WAVEFORM_FILE}...\")\n",
    "try:\n",
    "    long_df = pd.read_csv(LONG_WAVEFORM_FILE)\n",
    "    print(f\"Successfully loaded. Shape: {long_df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Input file not found at {LONG_WAVEFORM_FILE}\")\n",
    "    print(\"Please ensure the preparation script has been run and the file exists.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# 5.2 Pivot to Wide Format\n",
    "print(\"Pivoting waveform data from long to wide format...\")\n",
    "id_cols = ['caseid', TARGET_COLUMN]\n",
    "pivot_col = 'waveform'\n",
    "feature_cols = [col for col in long_df.columns if col not in id_cols + [pivot_col]]\n",
    "\n",
    "if not feature_cols:\n",
    "    print(\"ERROR: No waveform feature columns found. Check your input CSV.\")\n",
    "    raise ValueError(\"No feature columns detected to pivot.\")\n",
    "\n",
    "waveform_wide_df = long_df.pivot_table(\n",
    "    index=id_cols,\n",
    "    columns=pivot_col,\n",
    "    values=feature_cols\n",
    ")\n",
    "print(f\"Pivot complete. Shape before flattening: {waveform_wide_df.shape}\")\n",
    "\n",
    "# 5.3 Flatten and Clean Column Names\n",
    "print(\"Flattening and cleaning column names...\")\n",
    "new_cols = []\n",
    "for feature_name, waveform_name in waveform_wide_df.columns:\n",
    "    # Replace slashes (e.g., in 'SNUADC/PLETH') with underscores\n",
    "    clean_waveform = waveform_name.replace('/', '_')\n",
    "    new_cols.append(f\"{clean_waveform}_{feature_name}\")\n",
    "\n",
    "waveform_wide_df.columns = new_cols\n",
    "waveform_wide_df = waveform_wide_df.reset_index()\n",
    "print(f\"Column names flattened. Shape after reset_index: {waveform_wide_df.shape}\")\n",
    "\n",
    "# 5.4 Impute Waveform Data\n",
    "print(\"Imputing missing waveform values (from missing windows) with 0...\")\n",
    "nan_count_before = waveform_wide_df.isna().sum().sum()\n",
    "waveform_wide_df.fillna(0, inplace=True)\n",
    "print(f\"Total NaN values filled: {nan_count_before}\")\n",
    "\n",
    "# --- 6. Final Merge and Save ---\n",
    "print(\"\\n--- Merging Processed Data ---\")\n",
    "print(f\"Merging waveform data (shape: {waveform_wide_df.shape}) with preop data (shape: {preop_final_df.shape})...\")\n",
    "\n",
    "# Use a left merge to ensure we keep all patients from the waveform cohort\n",
    "# and attach their cleaned preoperative data.\n",
    "master_df = pd.merge(\n",
    "    waveform_wide_df,\n",
    "    preop_final_df,\n",
    "    on=['caseid', TARGET_COLUMN],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Check for any NaNs introduced by the merge (e.g., a caseid in waveform file\n",
    "# but not in preop file, which shouldn't happen if cohort file is the source)\n",
    "merge_nan_count = master_df.isnull().sum().sum()\n",
    "if merge_nan_count > 0:\n",
    "    print(f\"WARNING: Merge introduced {merge_nan_count} NaN values. This may indicate a mismatch.\")\n",
    "    print(\"Imputing these with -99 (assuming they are missing preop features)...\")\n",
    "    master_df.fillna(-99, inplace=True)\n",
    "\n",
    "print(f\"Final master DataFrame shape: {master_df.shape}\")\n",
    "\n",
    "# --- 7. Save Master File ---\n",
    "print(f\"\\nSaving new master wide-format file to {WIDE_FEATURES_FILE}...\")\n",
    "master_df.to_csv(WIDE_FEATURES_FILE, index=False)\n",
    "\n",
    "print(\"---\")\n",
    "print(\"✅ Comprehensive data wrangler script complete.\")\n",
    "print(\"\\n--- Final Output Sample (first 7 columns) ---\")\n",
    "print(master_df.iloc[:, :7].head())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aki_prediction_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
